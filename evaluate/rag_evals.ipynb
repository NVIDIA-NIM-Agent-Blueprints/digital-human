{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and store responses from the deployed RAG pipeline\n",
    "\n",
    "Once you have deployed the pipeline, we can evaluate the performances of the following setups:\n",
    "1. Without Knowledge Base\n",
    "2. With Knowledge Base\n",
    "3. With Knowledge Base and customized PEFT weights\n",
    "\n",
    "We will start with functions to parse the responses from our deployment, get_rag_response and get_rag_context (when knowledge base is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please ensure you have all the requirements installed before proceeding\n",
    "# !pip install -r requirements.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "## Get response of a given question from the RAG pipeline endpoint\n",
    "def get_rag_response(RAG_URL, question, use_knowledge_base, temperature=0.1, top_p=0.7, max_tokens=1024):\n",
    "    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                }\n",
    "            ],\n",
    "            \"use_knowledge_base\": use_knowledge_base,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stop\": []\n",
    "        }\n",
    "    response = requests.post(RAG_URL+\"generate\", headers=headers, json=data, stream=True, timeout=60)\n",
    "    if response.status_code == 200:\n",
    "            # Store the full response from the incoming response stream\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if len(line.strip())==0:\n",
    "                    continue\n",
    "                line = line.decode('utf-8').split(\"data: \")[1]\n",
    "                line = json.loads(line)[\"choices\"][0][\"message\"]['content']\n",
    "                full_response += line\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return response.text\n",
    "    return full_response\n",
    "\n",
    "## Get context retrieved for a given question from the RAG pipeline endpoint\n",
    "def get_rag_context(RAG_URL, question, top_k=4):\n",
    "    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "            \"query\": question,\n",
    "            \"top_k\": top_k\n",
    "            }\n",
    "    response = requests.post(RAG_URL+\"search\", headers=headers, json=data, stream=True, timeout=60)\n",
    "    if response.status_code == 200:\n",
    "            # Store the full response from the incoming response stream\n",
    "            retrieved_context = \"\"\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                full_response += line.decode('utf-8')\n",
    "            # Concatenate all the retrieved chunks in the full response\n",
    "            for chunk in json.loads(full_response)[\"chunks\"]:\n",
    "                retrieved_context += chunk['content'] + \"\\n\\n\"\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return response.text\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "## Generate all the responses and retrieved context (if use_knowledge_base is True) for all the questions in a given dataset\n",
    "def generate_evals_dataset(DATASET, RAG_URL, use_knowledge_base=True):\n",
    "    evals_list = []\n",
    "    with open(TEST_DATASET, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_lines = len(lines)\n",
    "        for idx, line in enumerate(tqdm(lines, total=num_lines, desc=\"Processing\", unit=\"questions\")):\n",
    "            eval_dict = {}\n",
    "            line = json.loads(line)\n",
    "            ## Save gold truth from annotated test dataset\n",
    "            gold_truth_context = line['input'].split(\"Question: \")[0].strip()\n",
    "            question = line['input'].split(\"Question: \")[1].strip()\n",
    "            gold_truth_answer = line['output']\n",
    "\n",
    "            ## Get RAG answers and retrieved context for the question\n",
    "            retrieved_answer = get_rag_response(RAG_URL, question, use_knowledge_base)\n",
    "            if use_knowledge_base==True:\n",
    "                retrieved_context = get_rag_context(RAG_URL, question)\n",
    "            else:\n",
    "                retrieved_context = \"\"\n",
    "\n",
    "            evals_list.append({\"question\" : question,\n",
    "                                \"rag_answer\": retrieved_answer,\n",
    "                                \"rag_context\": retrieved_context,\n",
    "                                \"gt_answer\": gold_truth_answer,\n",
    "                                \"gt_context\": gold_truth_context})\n",
    "    return evals_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the different setups, we are using the data/test_set.jsonl file we created during the PEFT process.\n",
    "\n",
    "Please update the variables below with the respective values, namely the NVIDIA_API_KEY, IP_ADDRESS and \n",
    "CHAIN_SERVER_PORT (8081 by default, and can be found in deploy/docker-compose.yaml file).\n",
    "\n",
    "The NVIDIA_API_KEY is your [NGC personal API key](https://org.ngc.nvidia.com/setup/personal-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY=\"../data/\"\n",
    "TEST_DATASET = DATASET_DIRECTORY+\"test_set.jsonl\"\n",
    "\n",
    "## Fill in the values of NVIDIA_API_KEY, IP_ADDRESS and CHAIN_SERVER_PORT:\n",
    "NVIDIA_API_KEY = \"nvapi--***\"\n",
    "IP_ADDRESS = \"\"\n",
    "CHAIN_SERVER_PORT = \"\"\n",
    "\n",
    "RAG_URL = f\"http://{IP_ADDRESS}:{CHAIN_SERVER_PORT}/\"\n",
    "\n",
    "## Generate answers without external RAG vectorDB\n",
    "print(\"Generating answers without knowledge base\")\n",
    "no_rag_responses = generate_evals_dataset(TEST_DATASET, RAG_URL=RAG_URL, use_knowledge_base=False)\n",
    "\n",
    "## Generate answers with external RAG vectorDB\n",
    "print(\"Generating answers with knowledge base\")\n",
    "rag_responses = generate_evals_dataset(TEST_DATASET, RAG_URL=RAG_URL, use_knowledge_base=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have saved our responses for the first two setups (base generator LLM with and without Knowledge Base), we will redo the same for our RAG deployment pipeline with PEFT weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the values of PEFT_IP_ADDRESS and PEFT_CHAIN_SERVER_PORT:\n",
    "PEFT_IP_ADDRESS = \"\"\n",
    "PEFT_CHAIN_SERVER_PORT = \"\"\n",
    "\n",
    "PEFT_RAG_URL = f\"http://{PEFT_IP_ADDRESS}:{PEFT_CHAIN_SERVER_PORT}/\"\n",
    " \n",
    "## Generate answers with external RAG vectorDB and PEFT weights\n",
    "rag_peft_responses = generate_evals_dataset(TEST_DATASET, RAG_URL=PEFT_RAG_URL, use_knowledge_base=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have saved the responses from different setups, we will the following metrics to evaluate the responses:\n",
    "1. ROUGE Score\n",
    "2. BLEU Score\n",
    "3. RAGAS (context precision, context relevancy, answer similarity, answer relevancy, faithfulness)\n",
    "4. LLM-as-Judge (answer similarity, , answer relevancy, answer correctness, faithfulness, overall score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE and BLEU scores\n",
    "\n",
    "[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) is a set of metrics primarily used for evaluating text generation. ROUGE-N refers to the overlap of n-grams between the generated text and a reference text. For example, ROUGE-1 and ROUGE-2 evaluate the overlap of unigrams and bigrams, respectively. On the other hand, ROUGE-L focuses on the longest common subsequence (LCS) between the generated text and the reference, capturing fluency and syntactical correctness.\n",
    "\n",
    "[BLEU](https://huggingface.co/spaces/evaluate-metric/bleu) is another popular metric for evaluating the quality of generated text. It measures how many words or phrases in the generated text match the reference text, considering n-gram overlaps up to a certain length. BLEU typically uses a precision-based approach and includes a brevity penalty to discourage overly short translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the answers from different RAGs and references (gold truth answer)\n",
    "rag_peft_predictions = [data['rag_answer'] for data in rag_peft_responses]\n",
    "rag_predictions = [data['rag_answer'] for data in rag_responses]\n",
    "no_rag_predictions = [data['rag_answer'] for data in no_rag_responses]\n",
    "references = [[data['gt_answer']] for data in no_rag_responses]\n",
    "\n",
    "# Load and compute BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rag_peft_bleu = bleu.compute(predictions=rag_peft_predictions, references=references)\n",
    "rag_bleu = bleu.compute(predictions=rag_predictions, references=references)\n",
    "no_rag_bleu = bleu.compute(predictions=no_rag_predictions, references=references)\n",
    "\n",
    "\n",
    "# Load and compute ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "rag_peft_rouge = rouge.compute(predictions=rag_peft_predictions, references=references)\n",
    "rag_rouge = rouge.compute(predictions=rag_predictions, references=references)\n",
    "no_rag_rouge = rouge.compute(predictions=no_rag_predictions, references=references)\n",
    "\n",
    "\n",
    "## Visualize the BLEU and ROUGE evaluation in tabular form\n",
    "metrics_data = {\n",
    "    'Metrics': ['BLEU Score', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-Lsum'],\n",
    "    'With RAG+PEFT Weights': [rag_peft_bleu['bleu'], rag_peft_rouge['rouge1'], rag_peft_rouge['rouge2'], rag_peft_rouge['rougeL'], rag_peft_rouge['rougeLsum']],\n",
    "    'With RAG': [rag_bleu['bleu'], rag_rouge['rouge1'], rag_rouge['rouge2'], rag_rouge['rougeL'], rag_rouge['rougeLsum']],\n",
    "    'Without RAG': [no_rag_bleu['bleu'], no_rag_rouge['rouge1'], no_rag_rouge['rouge2'], no_rag_rouge['rougeL'], no_rag_rouge['rougeLsum']]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ROUGE and BLEU are widely used automated techniques for evaluating text generation, they focus on surface-level similarity through n-gram overlap, often missing the semantics, coherence, and quality of the generated text. They might also overlook relevance, and factual accuracy, making them less suitable for detecting hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS\n",
    "\n",
    "[RAGAS](https://docs.ragas.io/) (Retrieval-Augmented Generation Alignment Score) is a metric specifically designed to evaluate Retrieval-Augmented Generation (RAG) models by assessing both the quality of the generated response and its alignment with the retrieved context. Unlike ROUGE and BLEU, which focus mainly on n-gram overlap and surface-level similarity, RAGAS considers the relevance and coherence of the generated text in relation to the retrieved information. This makes it more suitable for evaluating RAG models, as it better captures the model's ability to generate contextually appropriate and accurate responses.\n",
    "\n",
    "To judge the retrieval process, we will use the [context recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html) and [context precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html) metrics. For the generation process, we will use [answer similarity](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html), [answer relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html) and [faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import time\n",
    "from ragas.llms import LangchainLLM\n",
    "from ragas.metrics import (context_recall, context_precision, answer_relevancy, faithfulness, answer_correctness)\n",
    "from ragas.metrics import (Faithfulness, AnswerCorrectness, ContextRelevancy, AnswerRelevancy, AnswerSimilarity, ContextRecall, ContextPrecision)\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use NVIDIA AI Catalog's Llama3-70B-instruct LLM as a judge and eval model. NVIDIA AI Playground on NGC allows developers to experience state of the art LLMs accelerated on NVIDIA DGX Cloud with NVIDIA TensorRT nd Triton Inference Server. \n",
    "\n",
    "Developers get free credits for 4000 requests to any of the available models. \n",
    "\n",
    "If you have not signed up yet, don't worry! You can sign up here: https://build.nvidia.com/explore/discover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up RAGAS to use NVIDIA Catalog end points\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "LLM_MODEL = 'meta/llama3-70b-instruct'\n",
    "\n",
    "EMBED_MODEL = \"NV-Embed-QA\"\n",
    "os.environ['NVIDIA_API_KEY'] = NVIDIA_API_KEY\n",
    "\n",
    "## Initialize the NVIDIA LLM and embedder\n",
    "llm = ChatNVIDIA(model= LLM_MODEL,  max_tokens = 4000, nvidia_api_key=NVIDIA_API_KEY)\n",
    "nv_embedder = NVIDIAEmbeddings(model=EMBED_MODEL, truncate=\"END\", nvidia_api_key=NVIDIA_API_KEY)\n",
    "nv_document_embedder = NVIDIAEmbeddings(model=EMBED_MODEL, model_type=\"passage\", truncate=\"END\")\n",
    "nv_query_embedder = NVIDIAEmbeddings(model=EMBED_MODEL, model_type=\"query\", truncate=\"END\", nvidia_api_key=NVIDIA_API_KEY)\n",
    "nvpl_llm = LangchainLLM(llm=llm)\n",
    "\n",
    "## Initialize all RAGAS metrics with NVIDIA LLM and embedder\n",
    "answer_similarity = AnswerSimilarity(llm=nvpl_llm, embeddings=nv_query_embedder)\n",
    "answer_similarity.init_model()\n",
    "\n",
    "answer_relevancy = AnswerRelevancy(embeddings=nv_query_embedder,llm=nvpl_llm) #embeddings=nv_query_embedder,\n",
    "answer_relevancy.init_model()\n",
    "\n",
    "answer_correctness = AnswerCorrectness(llm=nvpl_llm, weights=[0.4,0.6])\n",
    "answer_correctness.init_model()\n",
    "\n",
    "faithfulness = Faithfulness(llm=nvpl_llm)\n",
    "faithfulness.init_model()\n",
    "\n",
    "context_recall = ContextRecall(llm=nvpl_llm)\n",
    "context_recall.init_model()\n",
    "\n",
    "context_precision = ContextPrecision(llm=nvpl_llm)\n",
    "context_precision.init_model()\n",
    "\n",
    "context_relevancy = ContextRelevancy(llm=nvpl_llm)\n",
    "context_relevancy.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to calculate RAGAS metric for given list of responses having question, response from RAG pipeline (rag_answer), \n",
    "#  retrieved context (rag_context) and ground truth answer (gt_answer) fields\n",
    "\n",
    "def get_ragas_results(evals_list):\n",
    "    ragas_results = {}\n",
    "    data_samples = {\n",
    "                    'question': [data[\"question\"] for data in evals_list],\n",
    "                    'answer': [data[\"rag_answer\"] for data in evals_list],\n",
    "                    'contexts' : [data[\"rag_context\"].split(\"\\n\\n\") for data in evals_list],\n",
    "                    'ground_truths': [[data[\"gt_answer\"]] for data in evals_list],}\n",
    "\n",
    "    data_samples = Dataset.from_dict(data_samples)\n",
    "\n",
    "    ## If you need to store the ratings for each datapoint separately, convert the ragas_results into dataframe and store it:\n",
    "    ##    ragas_results = evaluate(data_samples, metrics=[x, y, z])\n",
    "    ##    ragas_results = ragas_results.to_pandas()\n",
    "    ragas_results = evaluate(data_samples,metrics=[context_precision, context_relevancy, answer_similarity, answer_relevancy, faithfulness])\n",
    "    return ragas_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rag_ragas = get_ragas_results(no_rag_responses)\n",
    "rag_ragas = get_ragas_results(rag_responses)\n",
    "rag_peft_ragas = get_ragas_results(rag_peft_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the RAGAS evaluation in tabular form\n",
    "Metrics = no_rag_ragas.keys()\n",
    "metrics_data = {\n",
    "    'Metrics': Metrics,\n",
    "    'With RAG+PEFT Weights': [rag_peft_ragas[metric] for metric in Metrics],\n",
    "    'With RAG': [rag_ragas[metric] for metric in Metrics],\n",
    "    'Without RAG': [no_rag_ragas[metric] for metric in Metrics]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-JUDGE\n",
    "\n",
    "The LLM-as-Judge method involves using a large language model (LLM) to evaluate the quality of generated responses by comparing them against reference answers or assessing their relevance to the context. This approach can be particularly powerful when evaluating complex tasks, as it leverages the LLM's understanding of language, context, and nuanced meanings, potentially offering a more holistic evaluation than metrics like RAGAS, ROUGE or BLEU.\n",
    "\n",
    "However, the LLM-as-Judge method may introduce biases inherent in the judging model and lacks explainibility, making it less reliable as a standalone metric. This is an active area of research, and the effectiveness of the LLM-as-Judge method can significantly depend on the prompts used and your specific use case. There is ongoing exploration into optimizing prompts for more accurate and fair assessments. Developers should carefully research and experiment with different prompts to determine the most suitable approach for their specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "## Get response of the LLM judge on the ratings of different RAG responses\n",
    "def generate_llm_as_judge_response(prompt, llm_model='meta/llama3-70b-instruct', max_tokens=4000):\n",
    "    llm = ChatNVIDIA(model= llm_model,  max_tokens = max_tokens, nvidia_api_key=NVIDIA_API_KEY)\n",
    "    langchain_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a fair evaluator of different Retrieval Augmented Generation (RAG) pipelines. You will be given a task to rank different answers from different RAG pipelines.\"), (\"user\", \"{prompt}\")])\n",
    "    chain = langchain_prompt | llm | StrOutputParser()\n",
    "    full_response = \"\"\n",
    "    for response in chain.stream({\"prompt\": prompt}):\n",
    "        full_response += response\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example prompt template for LLM-as-Judge to rate different RAG answers based on different user-defined metrics\n",
    "## Note that we pass the ground truth context and answer for reference\n",
    "\n",
    "ANSWER_EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "Here are the answers given by different RAG pipelines, along with the question, gold truth answer and gold truth context.\n",
    "\n",
    "###Gold Truth context:\n",
    "{gt_context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "###Gold Truth Answer (Score 5):\n",
    "{gt_answer}\n",
    "\n",
    "###The answer given by RAG pipeline 1:\n",
    "{rag_answer1}\n",
    "\n",
    "###The answer given by RAG pipeline 2:\n",
    "{rag_answer2}\n",
    "\n",
    "###The answer given by RAG pipeline 3:\n",
    "{rag_answer3}\n",
    "\n",
    "You should rate each of the answers out of 5 with the format: x/5. Give your rating on each of these metrics:\n",
    "1. Answer similarity: The answer given by the RAG pipeline is similar to the the gold truth answer. Note that the answer should be factually correct.\n",
    "2. Faithfulness: The information in the answer should be present in the context.\n",
    "3. Answer relevancy: The answer given is relevant to the question asked and the gold truth context.\n",
    "4. Answer correctness: Gauge the accuracy of the generated answer when compared to the ground truth. Be careful of hallucinations.\n",
    "5. Overall score of each answer\n",
    "\n",
    "Choose the best answer out of all answers. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse and store the responses of the LLM judge for each of the stored RAG responses\n",
    "\n",
    "metrics = [\"Answer similarity\", \"Faithfulness\", \"Answer relevancy\", \"Answer correctness\", \"Overall score\"]\n",
    "llm_as_judge_evals = {\"no_rag\":{metric:[] for metric in metrics}, \"rag\":{metric:[] for metric in metrics}, \"rag_peft\":{metric:[] for metric in metrics}}\n",
    "llm_as_judge_responses = []\n",
    "for idx in range(len(no_rag_responses)):\n",
    "    llm_eval = None\n",
    "    print(\"Generating evals \",idx+1, \"/\", len(no_rag_responses))\n",
    "    try:\n",
    "        ans_eval_prompt = ANSWER_EVALUATION_PROMPT.format(\n",
    "                rag_answer1=no_rag_responses[idx]['rag_answer'],\n",
    "                rag_answer2=rag_responses[idx]['rag_answer'],\n",
    "                rag_answer3=rag_peft_responses[idx]['rag_answer'],\n",
    "                gt_context=no_rag_responses[idx]['gt_context'],\n",
    "                gt_answer=no_rag_responses[idx]['gt_answer'],\n",
    "                question=no_rag_responses[idx]['question']\n",
    "            )\n",
    "        llm_eval = generate_llm_as_judge_response(prompt=ans_eval_prompt)\n",
    "        llm_as_judge_responses.append(llm_eval)\n",
    "        for metric in metrics:\n",
    "            llm_as_judge_evals[\"no_rag\"][metric].append(float(llm_eval.split(metric)[1].split(\" \")[1].split(\"/\")[0]))\n",
    "            llm_as_judge_evals[\"rag\"][metric].append(float(llm_eval.split(metric)[2].split(\" \")[1].split(\"/\")[0]))\n",
    "            llm_as_judge_evals[\"rag_peft\"][metric].append(float(llm_eval.split(metric)[3].split(\" \")[1].split(\"/\")[0]))\n",
    "    except:\n",
    "        idx -= 1\n",
    "        print(\"Error in parsing LLM response\", llm_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the LLM-as-Judge evaluation in tabular form\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "metrics_data = {\n",
    "    'Metrics': metrics,\n",
    "    'With RAG+PEFT Weights': [mean(llm_as_judge_evals['rag_peft'][metric]) for metric in metrics],\n",
    "    'With RAG': [mean(llm_as_judge_evals['rag'][metric]) for metric in metrics],\n",
    "    'Without RAG': [mean(llm_as_judge_evals['no_rag'][metric]) for metric in metrics]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(df_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
